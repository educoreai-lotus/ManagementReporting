# ×”×¡×‘×¨ ××œ× ×•××¤×•×¨×˜ ×¢×œ ×”××¢×¨×›×ª - ××”×”×ª×—×œ×” ×¢×“ ×”×¡×•×£

**××¡××š ×–×” × ×•×¢×“ ×œ×¨××™×•× ×•×ª ×¢×‘×•×“×” - ×”×¡×‘×¨ ××§×¡×™××œ×™ ×¢×œ ×›×œ ×ª×”×œ×™×š ×‘××¢×¨×›×ª**

---

## ğŸ“‹ ×ª×•×›×Ÿ ×¢× ×™×™× ×™×

1. [××¨×›×™×˜×§×˜×•×¨×” ×›×œ×œ×™×ª](#××¨×›×™×˜×§×˜×•×¨×”-×›×œ×œ×™×ª)
2. [×ª×”×œ×™×š ×”×¤×¢×œ×” (Startup)](#×ª×”×œ×™×š-×”×¤×¢×œ×”-startup)
3. [×ª×”×œ×™×š ×˜×¢×™× ×ª Dashboard - ××”×”×ª×—×œ×” ×¢×“ ×”×¡×•×£](#×ª×”×œ×™×š-×˜×¢×™× ×ª-dashboard)
4. [×ª×”×œ×™×š ××™×¡×•×£ × ×ª×•× ×™× (Data Collection)](#×ª×”×œ×™×š-××™×¡×•×£-× ×ª×•× ×™×)
5. [×ª×”×œ×™×š ×™×¦×™×¨×ª Charts](#×ª×”×œ×™×š-×™×¦×™×¨×ª-charts)
6. [×ª×”×œ×™×š Chart Transcription ×¢× OpenAI](#×ª×”×œ×™×š-chart-transcription)
7. [×ª×”×œ×™×š Refresh Data](#×ª×”×œ×™×š-refresh-data)
8. [×ª×”×œ×™×š ×™×¦×™×¨×ª Reports](#×ª×”×œ×™×š-×™×¦×™×¨×ª-reports)
9. [×ª×”×œ×™×š AI Custom SQL](#×ª×”×œ×™×š-ai-custom-sql)

---

## ğŸ—ï¸ ××¨×›×™×˜×§×˜×•×¨×” ×›×œ×œ×™×ª

### ××‘× ×” ×”×¤×¨×•×™×§×˜

```
lotus_project/
â”œâ”€â”€ frontend/          # React + Vite
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/    # ×¨×›×™×‘×™ UI
â”‚   â”‚   â”œâ”€â”€ hooks/         # Custom hooks
â”‚   â”‚   â”œâ”€â”€ services/      # API calls
â”‚   â”‚   â””â”€â”€ context/       # React Context (Theme)
â”‚   â””â”€â”€ package.json
â”‚
â”œâ”€â”€ backend/           # Node.js + Express (Onion Architecture)
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ presentation/  # Routes, Controllers, Middleware
â”‚   â”‚   â”œâ”€â”€ application/   # Use Cases, Services
â”‚   â”‚   â”œâ”€â”€ domain/       # Entities, Value Objects
â”‚   â”‚   â””â”€â”€ infrastructure/ # DB, Clients, Jobs
â”‚   â””â”€â”€ package.json
â”‚
â””â”€â”€ DB/                # Database migrations & scripts
```

### Onion Architecture - ×”×¡×‘×¨

×”××¢×¨×›×ª ××©×ª××©×ª ×‘-**Onion Architecture** (××¨×›×™×˜×§×˜×•×¨×ª ×‘×¦×œ) - ×”×¤×¨×“×” ×‘×¨×•×¨×” ×‘×™×Ÿ ×©×›×‘×•×ª:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Presentation Layer                â”‚  â† Routes, Controllers, Middleware
â”‚   (××” ×”××©×ª××© ×¨×•××”)                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   Application Layer                 â”‚  â† Use Cases, Business Logic
â”‚   (××” ×”××¢×¨×›×ª ×¢×•×©×”)                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   Domain Layer                      â”‚  â† Entities, Business Rules
â”‚   (×”×œ×•×’×™×§×” ×”×¢×¡×§×™×ª)                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   Infrastructure Layer              â”‚  â† Database, External APIs, Jobs
â”‚   (××™×š ×–×” ××ª×—×‘×¨ ×œ×¢×•×œ× ×”×—×™×¦×•×Ÿ)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**×™×ª×¨×•× ×•×ª:**
- ×”×¤×¨×“×” ×‘×¨×•×¨×” ×©×œ ××—×¨×™×•×ª
- ×§×œ ×œ×‘×“×•×§ (testing)
- ×§×œ ×œ×©× ×•×ª implementation (×œ××©×œ: Redis â†’ PostgreSQL)
- Business logic ×œ× ×ª×œ×•×™ ×‘-infrastructure

---

## ğŸš€ ×ª×”×œ×™×š ×”×¤×¢×œ×” (Startup)

### 1. Backend Startup - `backend/src/server.js`

**××” ×§×•×¨×” ×›×©×”×©×¨×ª ××ª×—×™×œ:**

```javascript
// ×©×•×¨×” 89-157: backend/src/server.js
app.listen(PORT, () => {
  console.log(`Server running on port ${PORT}`);
  
  // âš ï¸ CRITICAL: Run initialization in background
  // Railway healthcheck needs server to respond immediately
  (async () => {
    // 1. Run database migration
    await runMigration();
    
    // 2. Seed mock data if database is empty
    if (process.env.DATABASE_URL) {
      const isEmpty = await isDatabaseEmpty();
      if (isEmpty) {
        await seedMockData();
      }
    }
    
    // 3. Test database connection
    const health = await healthCheck();
    
    // 4. Check and fix permissions
    await checkAndFixPermissions();
    
    // 5. Initialize scheduled jobs (CRON)
    await initializeJobs();
  })();
});
```

**×ª×”×œ×™×š ××¤×•×¨×˜:**

1. **×”×’×“×¨×ª Middleware:**
   - ×©×•×¨×” 30-35: Security Headers
   - ×©×•×¨×” 38: CORS
   - ×©×•×¨×” 41-49: JSON Body Parser (×¢×“ 50MB)
   - ×©×•×¨×” 63: Rate Limiting
   - ×©×•×¨×” 64: Audit Logging

2. **×”×’×“×¨×ª Routes:**
   ```javascript
   // ×©×•×¨×” 76-83
   app.use('/api/v1/dashboard', dashboardRoutes);
   app.use('/api/v1/reports', reportsRoutes);
   app.use('/api/v1/data', dataRoutes);
   app.use('/api/v1/openai', openaiRoutes);
   app.use('/api/v1/ai', chartTranscriptionRoutes);
   app.use('/api/ai-custom', aiCustomRoutes);
   ```

3. **××ª×—×•×œ Jobs (CRON):**
   - `backend/src/infrastructure/jobs/index.js`
   - ×©×•×¨×” 12-31: ××ª×—×•×œ ×›×œ ×”-jobs
   - ×›×œ job ×¨×¥ ×‘-06:00 ×‘×‘×•×§×¨ (Asia/Jerusalem)

### 2. Frontend Startup - `frontend/src/main.jsx`

```javascript
// frontend/src/main.jsx
ReactDOM.createRoot(document.getElementById('root')).render(
  <React.StrictMode>
    <App />
  </React.StrictMode>
);
```

**×ª×”×œ×™×š:**
1. ReactDOM ×™×•×¦×¨ root
2. ×˜×•×¢×Ÿ `App.jsx`
3. `App.jsx` ××’×“×™×¨ Routes:
   ```javascript
   // frontend/src/App.jsx
   <Routes>
     <Route path="/" element={<Navigate to="/dashboard" replace />} />
     <Route path="/dashboard" element={<Dashboard />} />
     <Route path="/reports" element={<ReportsPage />} />
     <Route path="/ai-custom" element={<AICustomPage />} />
   </Routes>
   ```

---

## ğŸ“Š ×ª×”×œ×™×š ×˜×¢×™× ×ª Dashboard

### Flow ××œ× - ××”×”×ª×—×œ×” ×¢×“ ×”×¡×•×£

```
User opens browser
    â†“
React Router navigates to /dashboard
    â†“
DashboardContainer component mounts
    â†“
useDashboardData hook runs
    â†“
fetchDashboard() called
    â†“
Check browser cache (localStorage)
    â†“
If cache exists â†’ Show cached data immediately
    â†“
Fetch fresh data from backend in background
    â†“
GET /api/v1/dashboard
    â†“
DashboardController.getDashboard()
    â†“
GetDashboardUseCase.execute()
    â†“
CacheRepository.getLatestEntries()
    â†“
Query PostgreSQL database
    â†“
Format data into ChartData entities
    â†“
Return JSON to frontend
    â†“
Frontend updates state
    â†“
Charts render with Recharts
    â†“
Startup transcription flow begins
    â†“
Capture all charts (visible + hidden)
    â†“
Send to /api/v1/ai/chart-transcription/startup
    â†“
OpenAI generates transcriptions
    â†“
Save to database
    â†“
Done!
```

### 1. Frontend - `useDashboardData` Hook

**××™×§×•×:** `frontend/src/hooks/useDashboardData.js`

**×©×•×¨×” 107-401: `fetchDashboard()` function**

```javascript
const fetchDashboard = async (autoRefreshIfEmpty = false) => {
  // 1. Check persistent cache (localStorage)
  const persistentCached = browserCache.getPersistentData('dashboard');
  
  if (persistentCached && persistentCached.data?.charts?.length > 0) {
    // Show cached data immediately
    setData(persistentCached.data);
    setLastUpdated(persistentCached.lastUpdated);
    setLoading(false);
    
    // Fetch fresh data in background (after 5 seconds)
    setTimeout(async () => {
      const response = await dashboardAPI.getDashboard();
      // Update cache and state
    }, 5000);
  } else {
    // No cache - fetch fresh data
    const response = await dashboardAPI.getDashboard();
    const dashboardData = response.data;
    setData(dashboardData);
  }
  
  // 2. Startup transcription flow (only once per session)
  const isStartupDone = getStartupTranscriptionDone();
  if (!isStartupDone) {
    // Fetch ALL charts (priority + BOX)
    const allChartsResponse = await dashboardAPI.getAllCharts();
    const allChartsForTranscription = allChartsResponse.data?.charts || [];
    
    // Wait for charts to render
    await waitForChartsStartup(20, 500);
    
    // Build payloads and send to backend
    const chartsForStartup = allChartsForTranscription.map(chart => ({
      chartId: chart.id,
      context: chart.title,
      chartPayload: buildChartTranscriptionPayload(chart)
    }));
    
    // Send to startup endpoint
    await chartTranscriptionAPI.startup(chartsForStartup);
  }
};
```

**××” ×§×•×¨×” ×›××Ÿ:**
1. **×©×•×¨×” 115-127:** ×‘×•×“×§ cache ×‘-localStorage
2. **×©×•×¨×” 131-153:** ×× ×™×© cache, ××¦×™×’ ××™×“ ×•××¢×“×›×Ÿ ×‘×¨×§×¢
3. **×©×•×¨×” 166-173:** ×× ××™×Ÿ cache, ×©×•×œ×— ×‘×§×©×” ×œ×©×¨×ª
4. **×©×•×¨×” 229-360:** Startup transcription - ×¨×§ ×¤×¢× ××—×ª per session

### 2. API Call - `frontend/src/services/api.js`

**××™×§×•×:** `frontend/src/services/api.js`

```javascript
// ×©×•×¨×” 124-128
export const dashboardAPI = {
  getDashboard: () => api.get('/dashboard'),
  getAllCharts: () => api.get('/dashboard/all-charts'),
  refreshData: (services) => api.post('/dashboard/refresh', services?.length ? { services } : {}),
};
```

**Request Interceptor (×©×•×¨×” 33-81):**
- ××•×¡×™×£ JWT token ×× ×§×™×™×
- ×œ×•×’×™× ×¢×‘×•×¨ chart transcription endpoints

**Response Interceptor (×©×•×¨×” 84-121):**
- ××˜×¤×œ ×‘-401 (unauthorized) â†’ redirect to login
- ××˜×¤×œ ×‘-429 (rate limit) â†’ retry logic

### 3. Backend Route - `backend/src/presentation/routes/dashboard.js`

**××™×§×•×:** `backend/src/presentation/routes/dashboard.js`

```javascript
// ×©×•×¨×” 17
router.get('/', dashboardController.getDashboard.bind(dashboardController));
```

### 4. Controller - `backend/src/presentation/controllers/DashboardController.js`

**××™×§×•×:** `backend/src/presentation/controllers/DashboardController.js`

**×©×•×¨×” 38-145: `getDashboard()` method**

```javascript
async getDashboard(req, res, next) {
  try {
    // 1. Execute Use Case
    const dashboardData = await this.getDashboardUseCase.execute();
    const combinedAnalytics = await this.getCombinedAnalyticsUseCase.execute();
    
    // 2. Merge charts
    const allCharts = [
      ...dashboardData.charts,
      ...combinedAnalytics.charts
    ];
    
    // 3. Filter only priority charts for main dashboard
    const priorityCharts = allCharts.filter(chart => {
      // Include charts marked as priority
      if (chart.metadata?.isPriority === true) return true;
      // Exclude non-priority
      if (chart.metadata?.isPriority === false) return false;
      // Exclude detailed charts
      if (chart.metadata?.chartType) return false;
      // Exclude Content Studio (goes to BOX)
      if (chart.metadata?.service === 'contentStudio') return false;
      // Include priority services
      const priorityServices = ['directory', 'courseBuilder', 'assessment', 'learningAnalytics'];
      if (priorityServices.includes(chart.metadata?.service)) return true;
      return false;
    });
    
    // 4. If no data, auto-load mock data
    if (priorityCharts.length === 0) {
      const token = req.headers.authorization?.substring(7);
      await triggerManualCollection(token);
      // Retry fetching
      const retryDashboardData = await this.getDashboardUseCase.execute();
      // ...
    }
    
    // 5. Return response
    res.json({
      charts: priorityCharts,
      lastUpdated: dashboardData.lastUpdated
    });
  } catch (error) {
    // Error handling
    res.json({
      charts: this.getDefaultCharts(),
      lastUpdated: null
    });
  }
}
```

### 5. Use Case - `backend/src/application/useCases/GetDashboardUseCase.js`

**××™×§×•×:** `backend/src/application/useCases/GetDashboardUseCase.js`

**×©×•×¨×” 70-194: `execute()` method**

```javascript
async execute(latestEntries = null) {
  // 1. Get latest entries from cache repository
  if (!latestEntries) {
    latestEntries = await this.cacheRepository.getLatestEntries();
  }
  
  // 2. Process each service and create charts
  const charts = [];
  const priorityServices = ['directory', 'courseBuilder', 'assessment', 'learningAnalytics'];
  
  for (const { service, data } of latestEntries) {
    const config = SERVICE_CHART_CONFIG[service];
    
    // 3. Create main chart for priority services
    if (priorityServices.includes(service)) {
      const mainChartData = this.formatChartData(data, service, 'main');
      if (mainChartData.length > 0) {
        const mainChart = new ChartData({
          id: `chart-${service}`,
          title: config.title,
          type: config.type,
          data: mainChartData,
          description: config.description,
          metadata: {
            service,
            lastUpdated: data.metadata?.collected_at || null,
            colorScheme: config.colorScheme,
            isPriority: true
          }
        });
        charts.push(mainChart.toJSON());
      }
    }
    
    // 4. Create detailed charts for BOX
    const detailedCharts = this.createDetailedCharts(service, data, config.colorScheme);
    charts.push(...detailedCharts);
  }
  
  // 5. Return result
  return {
    charts,
    lastUpdated: charts
      .map((chart) => chart.metadata?.lastUpdated)
      .filter(Boolean)
      .sort((a, b) => new Date(b).getTime() - new Date(a).getTime())[0] || null
  };
}
```

**××” ×§×•×¨×” ×›××Ÿ:**
1. **×©×•×¨×” 74:** ×©×•×œ×£ ××ª ×”× ×ª×•× ×™× ×”××—×¨×•× ×™× ××”-cache
2. **×©×•×¨×” 93:** ×¢×•×‘×¨ ×¢×œ ×›×œ service (directory, courseBuilder, etc.)
3. **×©×•×¨×” 106:** ×™×•×¦×¨ main chart ×¢×‘×•×¨ priority services
4. **×©×•×¨×” 129:** ×™×•×¦×¨ detailed charts ×¢×‘×•×¨ BOX
5. **×©×•×¨×” 186:** ××—×–×™×¨ ××ª ×”×ª×•×¦××”

### 6. Repository - `backend/src/infrastructure/repositories/DatabaseAnalyticsRepository.js`

**××™×§×•×:** `backend/src/infrastructure/repositories/DatabaseAnalyticsRepository.js`

```javascript
async getLatestEntries() {
  // Query PostgreSQL for latest snapshot from each service
  const query = `
    SELECT 
      service,
      snapshot_data as data,
      snapshot_date as collected_at
    FROM (
      SELECT 
        service,
        snapshot_data,
        snapshot_date,
        ROW_NUMBER() OVER (PARTITION BY service ORDER BY snapshot_date DESC) as rn
      FROM learning_analytics_snapshot
      WHERE service IN ('directory', 'courseBuilder', 'assessment', 'contentStudio', 'learningAnalytics')
    ) ranked
    WHERE rn = 1
    ORDER BY service;
  `;
  
  const result = await pool.query(query);
  return result.rows.map(row => ({
    service: row.service,
    data: {
      metrics: row.data?.metrics || {},
      details: row.data?.details || {},
      metadata: {
        collected_at: row.collected_at,
        source: row.service,
        schema_version: '1.0'
      }
    }
  }));
}
```

**××” ×§×•×¨×” ×›××Ÿ:**
- ×©×•×œ×£ ××ª ×”-snapshot ×”××—×¨×•×Ÿ ××›×œ service
- ××©×ª××© ×‘-ROW_NUMBER() ×›×“×™ ×œ×§×‘×œ ×¨×§ ××ª ×”××—×¨×•×Ÿ
- ××—×–×™×¨ ××ª ×”× ×ª×•× ×™× ×‘×¤×•×¨××˜ ××—×™×“

### 7. Frontend Rendering - `frontend/src/components/Dashboard/DashboardContainer.jsx`

**××™×§×•×:** `frontend/src/components/Dashboard/DashboardContainer.jsx`

**×©×•×¨×” 71-77: Filtering charts**

```javascript
const priorityCharts = useMemo(() => {
  return allCharts?.filter((chart) => 
    chart.metadata?.isPriority !== false
  ) || [];
}, [allCharts, data?.charts]);

const boxCharts = useMemo(() => {
  return allCharts?.filter((chart) => 
    chart.metadata?.isPriority === false
  ) || [];
}, [allCharts]);
```

**×©×•×¨×” 191-195: Rendering**

```javascript
<ChartGrid
  charts={priorityCharts}
  onChartClick={handleChartClick}
  failedServices={failedServicesMap}
/>
```

**×©×•×¨×” 199-217: Hidden charts for transcription**

```javascript
{/* Hidden container to render ALL non-priority charts for transcription */}
{hiddenCharts.length > 0 && (
  <div
    aria-hidden="true"
    style={{
      position: 'absolute',
      top: '-9999px',
      left: '-9999px',
      width: '1200px',
      pointerEvents: 'none',
      opacity: 0,
    }}
  >
    <ChartGrid
      charts={hiddenCharts}
      onChartClick={() => {}}
      failedServices={failedServicesMap}
    />
  </div>
)}
```

**××” ×§×•×¨×” ×›××Ÿ:**
- Charts ×¢× `isPriority: true` â†’ ××•×¦×’×™× ×‘-main dashboard
- Charts ×¢× `isPriority: false` â†’ ××•×¦×’×™× ×‘-BOX sidebar
- Charts × ×¡×ª×¨×™× â†’ ××•×¦×’×™× ××—×•×¥ ×œ××¡×š ×›×“×™ ×©× ×™×ª×Ÿ ×™×”×™×” ×œ×ª×¤×•×¡ ××•×ª× ×œ×ª×¢×ª×•×§

---

## ğŸ“¥ ×ª×”×œ×™×š ××™×¡×•×£ × ×ª×•× ×™× (Data Collection) - CACHE BASED

### âš ï¸ ×—×©×•×‘: ×”××¢×¨×›×ª ×”×™× CACHE BASED

×”××¢×¨×›×ª **×œ×** ××©×ª××©×ª ×‘-Redis, ××œ× ×‘-**PostgreSQL ×›××¢×¨×›×ª Cache**:
- ×”× ×ª×•× ×™× × ×©××¨×™× ×›-**Snapshots** (×ª××•× ×•×ª ××¦×‘) ×‘-DB
- ×›×œ snapshot × ×©××¨ ×¢× `snapshot_date` (×ª××¨×™×š)
- ×”× ×ª×•× ×™× **×œ× real-time** - ×”× snapshots ×©× ××¡×¤×™× ×¤×¢× ×‘×™×•×
- ×–×” × ×§×¨× "Cache Based" ×›×™ ×”× ×ª×•× ×™× ×”× cached snapshots, ×œ× live data

### Flow ××œ× - ×××™×¡×•×£ ×¢×“ ×©××™×¨×”

```
CRON Job triggers (06:00 AM)
    â†“
Fetch data from Microservice
    â†“
Normalize data
    â†“
Save to PostgreSQL (Cache Tables)
    â†“
Query latest snapshot when needed
    â†“
Format for charts
    â†“
Return to frontend
```

### 1. Scheduled Jobs (CRON)

**××™×§×•×:** `backend/src/infrastructure/jobs/index.js`

**×©×•×¨×” 12-31: Initialize all jobs**

```javascript
export const initializeJobs = async () => {
  // 1. Daily collection at 07:00 AM
  initializeDailyCollection(JWT_TOKEN);
  
  // 2. Individual service syncs at 06:00 AM (Asia/Jerusalem)
  startContentStudioScheduler();
  startAssessmentScheduler();
  startCourseBuilderScheduler();
  startDirectoryScheduler();
  startLearningAnalyticsScheduler();
  
  // 3. Load initial mock data
  await loadInitialMockData();
};
```

### 2. Directory Job - ×“×•×’××” ××œ××”

**××™×§×•×:** `backend/src/infrastructure/jobs/DirectoryJob.js`

**×©×•×¨×” 9-37: `startDirectoryScheduler()`**

```javascript
export function startDirectoryScheduler() {
  // Run every day at 06:00 AM, timezone Asia/Jerusalem
  cron.schedule(
    "0 6 * * *",
    async () => {
      console.log("[CRON] Starting Directory sync at", new Date().toISOString());

      try {
        // 1. Fetch data from Directory microservice
        const companies = await fetchDirectoryDataFromService();

        // 2. Validate response
        if (!Array.isArray(companies) || companies.length === 0) {
          console.warn("[CRON] Directory sync returned an empty or non-array response");
        } else {
          // 3. Save to database cache
          await saveDirectorySnapshot(companies);
          console.log("[CRON] Directory sync finished successfully");
        }
      } catch (err) {
        console.error("[CRON] Directory sync failed:", err.message);
      }
    },
    {
      timezone: "Asia/Jerusalem"
    }
  );

  console.log("[CRON] Directory scheduler initialized - will run daily at 06:00");
}
```

**××” ×§×•×¨×” ×›××Ÿ:**
1. **×©×•×¨×” 12:** CRON ××ª×–××Ÿ ×œ×¨×•×¥ ×›×œ ×™×•× ×‘-06:00
2. **×©×•×¨×” 17:** ×§×•×¨× ×œ-`fetchDirectoryDataFromService()` - ×©×•×œ×— ×‘×§×©×” ×œ××™×§×¨×•×¡×¨×‘×™×¡
3. **×©×•×¨×” 22:** ×§×•×¨× ×œ-`saveDirectorySnapshot()` - ×©×•××¨ ×‘-DB

### 3. Daily Collection Job (Legacy - ×œ× ×‘×©×™××•×© ×¢×›×©×™×•)

**××™×§×•×:** `backend/src/infrastructure/jobs/DailyCollectionJob.js`

**×©×•×¨×” 9-56: `initializeDailyCollection()`**

```javascript
export const initializeDailyCollection = (jwtToken) => {
  const cacheRepository = getCacheRepository();
  const retryService = new RetryService();
  
  collectDataUseCase = new CollectDataUseCase(
    cacheRepository,
    retryService
  );
  
  // Schedule daily collection at 07:00 AM
  cron.schedule('0 7 * * *', async () => {
    console.log('Starting daily data collection at 07:00 AM');
    
    try {
      // Execute collection
      const results = await collectDataUseCase.execute(jwtToken);
      
      // Log results
      if (results.partial) {
        console.warn('Partial data collection - some services failed:', results.failed);
      }
    } catch (error) {
      console.error('Daily collection error:', error);
    }
  });
};
```

**âš ï¸ ×”×¢×¨×”:** ×–×” legacy code - ×‘×¤×•×¢×œ ×›×œ service ×™×© ×œ×• job × ×¤×¨×“ (DirectoryJob, CourseBuilderJob, etc.)

### 3. Collect Data Use Case

**××™×§×•×:** `backend/src/application/useCases/CollectDataUseCase.js`

**×©×•×¨×” 80-138: `execute()` method**

```javascript
async execute(jwtToken, services = null) {
  const serviceList = services?.length ? services : this.defaultServices;
  // defaultServices = ['directory', 'courseBuilder', 'assessment', 'contentStudio', 'learningAnalytics']
  
  const results = {
    successful: [],
    failed: [],
    partial: false,
    allFailed: false
  };
  
  // Process each service
  for (const service of serviceList) {
    const handler = this.serviceHandlers[service];
    
    try {
      // 1. Fetch data from microservice
      const data = await handler.fetch(jwtToken);
      
      // 2. Normalize data
      const normalized = this.normalizeData(data, service);
      
      // 3. Save to cache
      await this.cacheRepository.save(service, normalized);
      
      results.successful.push({
        service,
        lastUpdated: new Date().toISOString()
      });
    } catch (error) {
      // Handle errors
      results.failed.push({
        service,
        reason: error.message,
        lastSuccessful: null
      });
      results.partial = true;
    }
  }
  
  return results;
}
```

### 4. Service Clients - ××™×š ×©×•×œ×¤×™× × ×ª×•× ×™× ××”××™×§×¨×•×¡×¨×‘×™×¡×™×

**×“×•×’××”: Directory Client**

**××™×§×•×:** `backend/src/infrastructure/clients/DirectoryClient.js`

**×©×•×¨×” 42-98: `fetchDirectoryDataFromService()`**

```javascript
export async function fetchDirectoryDataFromService() {
  // 1. Build request object (format expected by Directory microservice)
  const requestObject = {
    requester_name: "ManagementReporting",
    payload: {},
    response: {
      companies: [
        {
          company_id: null,
          company_name: "",
          industry: "",
          company_size: "",
          date_registered: "",
          primary_hr_contact: "",
          approval_policy: "",
          decision_maker: "",
          kpis: null,
          max_test_attempts: null,
          website_url: "",
          verification_status: "",
          hierarchy: null
        }
      ]
    }
  };
  
  // 2. Send POST request to coordinator API
  const response = await axios.post(
    COORDINATOR_API_URL,  // e.g., "https://coordinator.educoreai.com"
    JSON.stringify(requestObject),
    {
      headers: { "Content-Type": "application/json" },
      timeout: 30000  // 30 seconds timeout
    }
  );
  
  // 3. Parse response
  // Directory service returns ONLY the "response" object as JSON string
  const parsed = typeof response.data === "string" 
    ? JSON.parse(response.data) 
    : response.data;
  
  // 4. Validate response structure
  if (!parsed.companies || !Array.isArray(parsed.companies)) {
    throw new Error("Expected Directory response to contain { companies: [...] }");
  }
  
  console.log(`[Directory Client] Received ${parsed.companies.length} companies`);
  
  // 5. Return companies array
  return parsed.companies;
}
```

**××” ×§×•×¨×” ×›××Ÿ:**
1. **×©×•×¨×” 44-65:** ×‘×•× ×” request object ×¢× template
2. **×©×•×¨×” 70-75:** ×©×•×œ×— POST request ×œ-coordinator API
3. **×©×•×¨×” 82-84:** ××¤×¨×¡×¨ ××ª ×”-response (JSON string ××• object)
4. **×©×•×¨×” 87-90:** ×‘×•×“×§ ×©×”×ª×©×•×‘×” ×ª×§×™× ×”
5. **×©×•×¨×” 93:** ××—×–×™×¨ array ×©×œ companies

**Flow:**
```
DirectoryJob (CRON)
    â†“
fetchDirectoryDataFromService()
    â†“
POST to COORDINATOR_API_URL
    â†“
Directory Microservice processes request
    â†“
Returns { companies: [...] }
    â†“
Parse and validate
    â†“
Return companies array
    â†“
saveDirectorySnapshot(companies)
```

### 5. Save to Database - ×ª×”×œ×™×š ×©××™×¨×” ××¤×•×¨×˜

**×”××¢×¨×›×ª ××©×ª××©×ª ×‘-PostgreSQL ×›××¢×¨×›×ª Cache, ×œ× Redis!**

#### 5.1. Directory Cache - ×“×•×’××” ××œ××”

**××™×§×•×:** `backend/src/infrastructure/db/directoryCache.js`

**×©×•×¨×” 13-97: `saveDirectorySnapshot()`**

```javascript
export async function saveDirectorySnapshot(dataArray) {
  if (!Array.isArray(dataArray)) {
    throw new Error("saveDirectorySnapshot expected dataArray to be an array");
  }

  const pool = getPool();
  const client = await pool.connect();

  // 1. Create snapshot date (YYYY-MM-DD format)
  const now = new Date();
  const snapshotDate = now.toISOString().slice(0, 10); // "2025-01-13"

  try {
    // 2. Begin transaction
    await client.query("BEGIN");

    // 3. Save each company as a snapshot row
    for (const data of dataArray) {
      await withRetry(async () => {
        return await client.query(
          `
          INSERT INTO directory_cache (
            snapshot_date,        -- ×ª××¨×™×š ×”-snapshot
            company_id,
            company_name,
            industry,
            company_size,
            date_registered,
            primary_hr_contact,
            approval_policy,
            decision_maker,
            kpis,                 -- JSONB field
            max_test_attempts,
            website_url,
            verification_status,
            hierarchy,            -- JSONB field
            ingested_at           -- timestamp ×©×œ ×”×©××™×¨×”
          )
          VALUES ($1,$2,$3,$4,$5,$6,$7,$8,$9,$10,$11,$12,$13,$14,$15)
          ON CONFLICT (snapshot_date, company_id)
          DO UPDATE SET
            company_name = EXCLUDED.company_name,
            industry = EXCLUDED.industry,
            company_size = EXCLUDED.company_size,
            -- ... update all fields
            ingested_at = EXCLUDED.ingested_at
          `,
          [
            snapshotDate,           // $1 - ×ª××¨×™×š ×”-snapshot
            data.company_id ?? "unknown",
            data.company_name ?? null,
            data.industry ?? null,
            data.company_size ?? null,
            data.date_registered ? new Date(data.date_registered) : null,
            data.primary_hr_contact ?? null,
            data.approval_policy ?? null,
            data.decision_maker ?? null,
            data.kpis ?? null,      // JSONB
            data.max_test_attempts ?? null,
            data.website_url ?? null,
            data.verification_status ?? null,
            data.hierarchy ?? null,  // JSONB
            now                     // ingested_at
          ]
        );
      }, 3); // Retry up to 3 times
    }

    // 4. Commit transaction
    await client.query("COMMIT");
    console.log(`[Directory Cache] âœ… Saved ${dataArray.length} directory snapshots for ${snapshotDate}`);
  } catch (err) {
    // 5. Rollback on error
    await client.query("ROLLBACK");
    console.error("[Directory Cache] âŒ Error saving directory snapshots to DB:", err.message);
    throw err;
  } finally {
    // 6. Release connection
    client.release();
  }
}
```

**××” ×§×•×¨×” ×›××Ÿ:**
1. **×©×•×¨×” 22:** ×™×•×¦×¨ snapshot date (YYYY-MM-DD)
2. **×©×•×¨×” 25:** ××ª×—×™×œ transaction
3. **×©×•×¨×” 27-83:** ×©×•××¨ ×›×œ company ×›-row × ×¤×¨×“ ×‘-`directory_cache`
4. **×©×•×¨×” 49:** `ON CONFLICT` - ×× ×›×‘×¨ ×™×© snapshot ×œ××•×ª×• ×ª××¨×™×š + company_id, ××¢×“×›×Ÿ
5. **×©×•×¨×” 86:** Commit transaction
6. **×©×•×¨×” 91:** Rollback ×× ×™×© ×©×’×™××”

**×˜×‘×œ×ª directory_cache:**
```sql
CREATE TABLE directory_cache (
  snapshot_date DATE,           -- ×ª××¨×™×š ×”-snapshot
  company_id VARCHAR,            -- ××–×”×” ×”×—×‘×¨×”
  company_name VARCHAR,
  industry VARCHAR,
  company_size VARCHAR,
  kpis JSONB,                    -- × ×ª×•× ×™× × ×•×¡×¤×™× ×‘-JSON
  hierarchy JSONB,                -- ×”×™×¨×¨×›×™×” ×‘-JSON
  ingested_at TIMESTAMP,          -- ××ª×™ × ×©××¨
  PRIMARY KEY (snapshot_date, company_id)  -- ××¤×ª×— ×¨××©×™
);
```

#### 5.2. Course Builder Cache

**××™×§×•×:** `backend/src/infrastructure/db/courseBuilderCache.js`

**×©×•×¨×” 12-80: `saveCourseBuilderSnapshots()`**

```javascript
export async function saveCourseBuilderSnapshots(courses) {
  const pool = getPool();
  const client = await pool.connect();
  
  const now = new Date();
  const snapshotDate = now.toISOString().slice(0, 10);

  try {
    await client.query("BEGIN");

    for (const course of courses) {
      await withRetry(async () => {
        return await client.query(
          `
          INSERT INTO course_builder_cache (
            snapshot_date,
            course_id,
            course_name,
            "totalEnrollments",
            "activeEnrollment",
            "completionRate",
            "averageRating",
            "createdAt",
            feedback,
            ingested_at
          )
          VALUES ($1,$2,$3,$4,$5,$6,$7,$8,$9,$10)
          ON CONFLICT (snapshot_date, course_id)
          DO UPDATE SET
            course_name = EXCLUDED.course_name,
            "totalEnrollments" = EXCLUDED."totalEnrollments",
            -- ... update all fields
            ingested_at = EXCLUDED.ingested_at
          `,
          [
            snapshotDate,
            course.course_id ?? "unknown",
            course.course_name ?? "Unknown Course",
            course.totalEnrollments ?? 0,
            course.activeEnrollment ?? 0,
            course.completionRate ?? 0,
            course.averageRating ?? 0,
            course.createdAt ? new Date(course.createdAt) : null,
            course.feedback ?? null,
            now
          ]
        );
      }, 3);
    }

    await client.query("COMMIT");
    console.log(`[Course Builder Cache] âœ… Saved ${courses.length} course snapshots for ${snapshotDate}`);
  } catch (err) {
    await client.query("ROLLBACK");
    throw err;
  } finally {
    client.release();
  }
}
```

#### 5.3. Assessment Cache

**××™×§×•×:** `backend/src/infrastructure/db/assessmentCache.js`

**×©×•×¨×” 11-74: `saveAssessmentSnapshot()`**

```javascript
export async function saveAssessmentSnapshot(dataArray) {
  const pool = getPool();
  const client = await pool.connect();
  
  const now = new Date();
  const snapshotDate = now.toISOString().slice(0, 10);

  try {
    await client.query("BEGIN");

    for (const row of dataArray) {
      await client.query(
        `
        INSERT INTO assessments_cache (
          snapshot_date,
          user_id,
          course_id,
          exam_type,
          attempt_no,
          passing_grade,
          final_grade,
          passed,
          ingested_at
        )
        VALUES ($1,$2,$3,$4,$5,$6,$7,$8,$9)
        ON CONFLICT (snapshot_date, user_id, course_id, exam_type, attempt_no)
        DO UPDATE SET
          passing_grade = EXCLUDED.passing_grade,
          final_grade = EXCLUDED.final_grade,
          passed = EXCLUDED.passed,
          ingested_at = EXCLUDED.ingested_at
        `,
        [
          snapshotDate,
          row.user_id ?? "unknown",
          row.course_id ?? "unknown",
          row.exam_type ?? "postcourse",
          row.attempt_no ?? 1,
          row.passing_grade ?? null,
          row.final_grade ?? null,
          row.passed ?? false,
          now
        ]
      );
    }

    await client.query("COMMIT");
    console.log(`[Assessment Cache] âœ… Saved snapshot for ${snapshotDate} with ${dataArray.length} rows`);
  } catch (err) {
    await client.query("ROLLBACK");
    throw err;
  } finally {
    client.release();
  }
}
```

#### 5.4. Learning Analytics Snapshot (××•×¨×›×‘ ×™×•×ª×¨)

**××™×§×•×:** `backend/src/infrastructure/db/learningAnalyticsCache.js`

**×©×•×¨×” 11-367: `saveLearningAnalyticsSnapshot()`**

```javascript
export async function saveLearningAnalyticsSnapshot(data) {
  const pool = getPool();
  const client = await pool.connect();
  
  const now = new Date();
  const snapshotDate = now.toISOString().slice(0, 10);

  try {
    await client.query("BEGIN");

    // 1. Insert main snapshot
    const snapshotResult = await client.query(
      `
      INSERT INTO public.learning_analytics_snapshot (
        snapshot_date,
        period,
        start_date,
        end_date,
        calculated_at
      )
      VALUES ($1, $2, $3, $4, $5)
      RETURNING id
      `,
      [
        snapshotDate,
        data.period || 'daily',
        data.start_date ? new Date(data.start_date) : null,
        data.end_date ? new Date(data.end_date) : null,
        now
      ]
    );
    
    const snapshotId = snapshotResult.rows[0].id;

    // 2. Insert learners data
    await client.query(
      `
      INSERT INTO public.learning_analytics_learners (
        snapshot_id,
        total_learners,
        active_learners,
        total_organizations
      )
      VALUES ($1, $2, $3, $4)
      `,
      [snapshotId, data.learners?.total || 0, data.learners?.active || 0, data.learners?.orgs || 0]
    );

    // 3. Insert courses data
    await client.query(
      `
      INSERT INTO public.learning_analytics_courses (
        snapshot_id,
        total_courses,
        courses_completed,
        average_completion_rate,
        total_enrollments,
        active_enrollments,
        average_course_duration_hours
      )
      VALUES ($1, $2, $3, $4, $5, $6, $7)
      `,
      [
        snapshotId,
        data.courses?.total || 0,
        data.courses?.completed || 0,
        data.courses?.avgCompletion || 0,
        data.courses?.enrollments || 0,
        data.courses?.activeEnrollments || 0,
        data.courses?.avgDuration || 0
      ]
    );

    // 4. Insert skills, engagement, breakdowns, etc.
    // ... (similar pattern)

    await client.query("COMMIT");
    console.log(`[Learning Analytics Cache] âœ… Saved snapshot ${snapshotId} for ${snapshotDate}`);
  } catch (err) {
    await client.query("ROLLBACK");
    throw err;
  } finally {
    client.release();
  }
}
```

**××” ××™×•×—×“ ×›××Ÿ:**
- ×™×© ×˜×‘×œ×” ×¨××©×™×ª: `learning_analytics_snapshot`
- ×™×© ×˜×‘×œ××•×ª ××©× ×”: `learning_analytics_learners`, `learning_analytics_courses`, etc.
- ×›×œ ×˜×‘×œ×ª ××©× ×” ××§×•×©×¨×ª ×œ-snapshot ×“×¨×š `snapshot_id` (Foreign Key)

### 6. ××™×š ×”× ×ª×•× ×™× × ×§×¨××™× ××”-Cache

**××™×§×•×:** `backend/src/infrastructure/repositories/DatabaseAnalyticsRepository.js`

**×©×•×¨×” 28-48: `getLatestByService()`**

```javascript
async getLatestByService(service) {
  try {
    switch (service) {
      case 'directory':
        return await this.fetchDirectoryData();
      case 'courseBuilder':
        return await this.fetchCourseBuilderData();
      // ... other services
    }
  } catch (error) {
    console.error(`Failed to load ${service}:`, error.message);
    return null;
  }
}
```

**×©×•×¨×” 168-258: `fetchDirectoryData()` - ×“×•×’××”**

```javascript
async fetchDirectoryData() {
  // 1. Query latest snapshot from directory_cache
  const { rows } = await this.pool.query(`
    SELECT *
    FROM public.directory_cache
    WHERE snapshot_date = (
      SELECT MAX(snapshot_date) FROM public.directory_cache
    )
  `);

  if (!rows.length) {
    return null;
  }

  // 2. Calculate metrics from raw data
  const orgUserMap = new Map();
  let totalUsers = 0;
  const usersByDepartment = {};

  for (const row of rows) {
    // Calculate based on company_size from DB
    const approxUsers = this.estimateUsersByCompanySize(row.company_size);
    totalUsers += approxUsers;
    orgUserMap.set(row.company_name, approxUsers);

    // Calculate departments from hierarchy JSONB
    const departments = row.hierarchy?.departments || [];
    if (departments.length) {
      const perDept = Math.max(1, Math.round(approxUsers / departments.length));
      departments.forEach((dept) => {
        usersByDepartment[dept] = (usersByDepartment[dept] || 0) + perDept;
      });
    }
  }

  // 3. Build metrics object
  const metrics = {
    totalUsers,              // Calculated from DB data
    totalOrganizations: rows.length,  // Direct count
    activeUsers: Math.round(totalUsers * 0.78),  // Estimated
    usersByDepartment,       // Calculated from hierarchy JSONB
    organizationsActive: rows.filter(r => r.verification_status === 'verified').length
  };

  // 4. Build details object
  const details = {
    organizations: rows.map((row) => ({
      company_id: row.company_id,
      company_name: row.company_name,
      industry: row.industry,
      company_size: row.company_size,
      hierarchy: row.hierarchy,  // JSONB from DB
      kpis: row.kpis            // JSONB from DB
    }))
  };

  // 5. Return formatted response
  return {
    timestamp: this.extractLatestTimestamp(rows),
    data: {
      metrics,
      details
    },
    metadata: {
      source: 'directory',
      schema_version: '1.0',
      collected_at: this.extractLatestTimestamp(rows)
    }
  };
}
```

**××” ×§×•×¨×” ×›××Ÿ:**
1. **×©×•×¨×” 169-175:** ×©×•×œ×£ ××ª ×”-snapshot ×”××—×¨×•×Ÿ (MAX snapshot_date)
2. **×©×•×¨×” 193-209:** ××—×©×‘ metrics ××”× ×ª×•× ×™× ×”×’×•×œ××™×™×
3. **×©×•×¨×” 232-239:** ×‘×•× ×” metrics object
4. **×©×•×¨×” 241-255:** ×‘×•× ×” details object
5. **×©×•×¨×” 257:** ××—×–×™×¨ ×‘×¤×•×¨××˜ ××—×™×“

### 7. ××“×•×¢ ×–×” × ×§×¨× "Cache Based"?

**×”×¡×‘×¨:**
- ×”× ×ª×•× ×™× **×œ× real-time** - ×”× snapshots ×©× ××¡×¤×™× ×¤×¢× ×‘×™×•×
- ×›×œ snapshot × ×©××¨ ×¢× `snapshot_date` - ××¤×©×¨ ×œ×¨××•×ª ×”×™×¡×˜×•×¨×™×”
- ×”× ×ª×•× ×™× × ×©××¨×™× ×‘-DB ××‘×œ **×›××• cache** - ×œ× ××¢×“×›× ×™× ×›×œ ×”×–××Ÿ
- ×›×©×¦×¨×™×š × ×ª×•× ×™×, ×©×•×œ×¤×™× ××ª ×”-snapshot ×”××—×¨×•×Ÿ

**×”×‘×“×œ ×-Redis:**
- **Redis:** In-memory cache, ××”×™×¨ ×××•×“, ××‘×œ ×œ× persistent
- **PostgreSQL Cache:** Persistent, ×™×›×•×œ ×œ×©××•×¨ ×”×™×¡×˜×•×¨×™×”, ××‘×œ ××™×˜×™ ×™×•×ª×¨

**×™×ª×¨×•× ×•×ª:**
- âœ… ×™×›×•×œ ×œ×©××•×¨ ×”×™×¡×˜×•×¨×™×” (60 ×™××™×)
- âœ… Persistent - ×œ× × ×¢×œ× ×× ×”×©×¨×ª × ×•×¤×œ
- âœ… ×™×›×•×œ ×œ×¢×©×•×ª queries ××•×¨×›×‘×™×
- âœ… ×™×›×•×œ ×œ×©××•×¨ JSONB (hierarchy, kpis)

**×—×¡×¨×•× ×•×ª:**
- âŒ ××™×˜×™ ×™×•×ª×¨ ×-Redis
- âŒ ×“×•×¨×© ×™×•×ª×¨ ××§×•× ×‘×“×™×¡×§
- âŒ ×œ× real-time (snapshots)

### 8. ×ª×”×œ×™×š ××œ× - ×××™×¡×•×£ ×¢×“ ×§×¨×™××”

#### 8.1. ×ª×”×œ×™×š ×”×©××™×¨×” (Write Flow)

```
06:00 AM - CRON Job triggers
    â†“
DirectoryJob.startDirectoryScheduler()
    â†“
fetchDirectoryDataFromService()
    â†“
    â”œâ”€ Build request object
    â”œâ”€ POST to COORDINATOR_API_URL
    â”œâ”€ Wait for response (timeout: 30s)
    â””â”€ Parse JSON response
    â†“
Receive companies array (e.g., 50 companies)
    â†“
saveDirectorySnapshot(companies)
    â†“
    â”œâ”€ Get database connection pool
    â”œâ”€ Create snapshot_date (YYYY-MM-DD)
    â””â”€ BEGIN TRANSACTION
    â†“
For each company (50 iterations):
    â”œâ”€ INSERT INTO directory_cache
    â”‚   VALUES (snapshot_date, company_id, ...)
    â”‚   ON CONFLICT (snapshot_date, company_id)
    â”‚   DO UPDATE SET ...
    â””â”€ Retry up to 3 times if fails
    â†“
COMMIT TRANSACTION
    â†“
Release connection
    â†“
Done! 50 rows saved to directory_cache
```

**×“×•×’××” ×§×•× ×§×¨×˜×™×ª:**
```
Date: 2025-01-13 06:00:00
Snapshot Date: "2025-01-13"

Companies received: 50

Transaction:
  INSERT INTO directory_cache (snapshot_date='2025-01-13', company_id='COMP-001', ...)
  INSERT INTO directory_cache (snapshot_date='2025-01-13', company_id='COMP-002', ...)
  ...
  INSERT INTO directory_cache (snapshot_date='2025-01-13', company_id='COMP-050', ...)

Result: 50 rows in directory_cache with snapshot_date='2025-01-13'
```

#### 8.2. ×ª×”×œ×™×š ×”×§×¨×™××” (Read Flow)

```
User opens dashboard
    â†“
GET /api/v1/dashboard
    â†“
DashboardController.getDashboard()
    â†“
GetDashboardUseCase.execute()
    â†“
DatabaseAnalyticsRepository.getLatestEntries()
    â†“
getLatestByService('directory')
    â†“
fetchDirectoryData()
    â†“
    â”œâ”€ Query: SELECT * FROM directory_cache
    â”‚   WHERE snapshot_date = (
    â”‚     SELECT MAX(snapshot_date) FROM directory_cache
    â”‚   )
    â””â”€ Returns: All rows with latest snapshot_date
    â†“
Calculate metrics from rows:
    â”œâ”€ totalUsers = sum(estimateUsersByCompanySize(company_size))
    â”œâ”€ totalOrganizations = rows.length
    â”œâ”€ activeUsers = calculate from kpis or estimate
    â”œâ”€ usersByDepartment = calculate from hierarchy JSONB
    â””â”€ organizationsActive = count(verification_status='verified')
    â†“
Build response:
    {
      timestamp: "2025-01-13T06:00:00Z",
      data: {
        metrics: { totalUsers: 5000, ... },
        details: { organizations: [...] }
      },
      metadata: {
        source: "directory",
        collected_at: "2025-01-13T06:00:00Z"
      }
    }
    â†“
Format as ChartData entities
    â†“
Return JSON to frontend
    â†“
Frontend renders charts
```

**×“×•×’××” ×§×•× ×§×¨×˜×™×ª:**
```
Query: SELECT * FROM directory_cache WHERE snapshot_date = '2025-01-13'
Result: 50 rows

Calculation:
  Row 1: company_size='50-200' â†’ estimateUsersByCompanySize() â†’ 125 users
  Row 2: company_size='200-500' â†’ 350 users
  ...
  Total: 5000 users

Metrics:
  totalUsers: 5000
  totalOrganizations: 50
  activeUsers: 3900 (78% of total)
  organizationsActive: 45 (verified)
```

### 9. ××“×•×¢ PostgreSQL ×•×œ× Redis?

**×”×¡×‘×¨ ×˜×›× ×™:**

**Redis (×‘××§×•×¨ ×ª×•×›× ×Ÿ):**
- âœ… ××”×™×¨ ×××•×“ (in-memory)
- âœ… TTL ××•×˜×•××˜×™
- âŒ ×œ× persistent (× ×¢×œ× ×× ×”×©×¨×ª × ×•×¤×œ)
- âŒ ×œ× ×™×›×•×œ ×œ×¢×©×•×ª queries ××•×¨×›×‘×™×
- âŒ ×œ× ×™×›×•×œ ×œ×©××•×¨ JSONB structures

**PostgreSQL (××” ×©×‘×¤×•×¢×œ):**
- âœ… Persistent - ×”× ×ª×•× ×™× × ×©××¨×™× ×‘×“×™×¡×§
- âœ… ×™×›×•×œ ×œ×¢×©×•×ª queries ××•×¨×›×‘×™× (JOIN, GROUP BY, etc.)
- âœ… ×ª×•××š ×‘-JSONB (hierarchy, kpis)
- âœ… ×™×›×•×œ ×œ×©××•×¨ ×”×™×¡×˜×•×¨×™×” (60 ×™××™×)
- âœ… Foreign Keys ×•-Constraints
- âŒ ××™×˜×™ ×™×•×ª×¨ ×-Redis (××‘×œ ×¢×“×™×™×Ÿ ××”×™×¨ ××¡×¤×™×§)

**×œ××” ×©×™× ×• ×œ-PostgreSQL:**
1. ×¦×¨×™×š ×œ×©××•×¨ ×”×™×¡×˜×•×¨×™×” (60 ×™××™×)
2. ×¦×¨×™×š ×œ×¢×©×•×ª queries ××•×¨×›×‘×™× (JOIN ×‘×™×Ÿ ×˜×‘×œ××•×ª)
3. ×¦×¨×™×š ×œ×©××•×¨ JSONB (hierarchy, kpis)
4. ×¦×¨×™×š persistent storage (×œ× ×¨×•×¦×™× ×œ××‘×“ × ×ª×•× ×™×)

**×”×˜×‘×œ××•×ª ×‘×¤×•×¢×œ:**
```sql
-- Cache tables (snapshots)
directory_cache          -- Companies snapshots
course_builder_cache     -- Courses snapshots
assessments_cache        -- Assessments snapshots
learning_analytics_snapshot  -- Main snapshot table
learning_analytics_learners  -- Related to snapshot
learning_analytics_courses   -- Related to snapshot
-- ... more related tables

-- Content Studio (normalized, not snapshots)
courses                  -- Master courses table
topics                   -- Master topics table
contents                 -- Master contents table
course_topics            -- Many-to-many relationship
```

**×”×‘×“×œ ×‘×™×Ÿ Cache Tables ×œ-Master Tables:**
- **Cache Tables:** Snapshots ×¢× `snapshot_date` - × ×ª×•× ×™× ×™×©× ×™× × ×©××¨×™×
- **Master Tables:** ××¢×•×“×›× ×™× ×›×œ ×”×–××Ÿ - ×¨×§ ×”× ×ª×•× ×™× ×”××—×¨×•× ×™×

### 10. Cleanup - ××—×™×§×ª × ×ª×•× ×™× ×™×©× ×™×

**××™×§×•×:** `DB/migration.sql`

```sql
-- Delete snapshots older than 60 days
DELETE FROM public.learning_analytics_snapshot 
WHERE snapshot_date < CURRENT_DATE - 60;

-- Cascade delete will remove related rows automatically
-- (because of ON DELETE CASCADE in foreign keys)
```

**××™×š ×–×” ×¢×•×‘×“:**
- ×›×œ ×™×•× ×‘-06:00, ×”-CRON job ×©×•××¨ snapshot ×—×“×©
- ×›×œ 60 ×™×•×, ×”× ×ª×•× ×™× ×”×™×©× ×™× × ××—×§×™× ××•×˜×•××˜×™×ª
- ×–×” × ×§×¨× "Rolling Window" - ×—×œ×•×Ÿ ×©×œ 60 ×™××™×

**×“×•×’××”:**
```
Today: 2025-01-13
Keep: 2024-11-14 to 2025-01-13 (60 days)
Delete: Everything before 2024-11-14
```

---

## ğŸ“ˆ ×ª×”×œ×™×š ×™×¦×™×¨×ª Charts

### 1. Format Chart Data

**××™×§×•×:** `backend/src/application/useCases/GetDashboardUseCase.js`

**×©×•×¨×” 196-301: `formatChartData()` and helpers**

```javascript
formatChartData(entry, service, chartType = 'main') {
  // 1. Extract metrics
  let metrics = entry?.data?.metrics || entry?.metrics || {};
  
  // 2. Format based on chart type
  if (chartType === 'main') {
    return this.formatMainChartData(service, metrics);
  }
  
  return this.formatDetailedChartData(service, metrics, chartType, entry);
}

formatMainChartData(service, metrics) {
  // Select key metrics for main chart
  const keyMetricsMap = {
    directory: ['totalUsers', 'totalOrganizations', 'activeUsers'],
    courseBuilder: ['totalCourses', 'totalEnrollments', 'averageCompletionRate'],
    assessment: ['totalAssessments', 'averageScore', 'passRate'],
    // ...
  };
  
  const keyMetrics = keyMetricsMap[service] || Object.keys(metrics).slice(0, 5);
  
  // Format as array for chart
  return Object.entries(keyMetrics)
    .filter(([key]) => metrics[key] !== undefined && typeof metrics[key] === 'number')
    .map(([key, value]) => ({
      name: formatMetricName(key),
      value: Math.round(value * 100) / 100
    }));
}
```

### 2. Create Chart Entity

**××™×§×•×:** `backend/src/domain/entities/ChartData.js`

```javascript
export class ChartData {
  constructor({ id, title, type, data, description, metadata }) {
    this.id = id;
    this.title = title;
    this.type = type; // 'bar', 'line', 'pie', 'area'
    this.data = data; // Array of { name, value } or { name, series1, series2, ... }
    this.description = description;
    this.metadata = metadata;
  }
  
  toJSON() {
    return {
      id: this.id,
      title: this.title,
      type: this.type,
      data: this.data,
      description: this.description,
      metadata: this.metadata
    };
  }
}
```

### 3. Frontend Chart Rendering

**××™×§×•×:** `frontend/src/components/Charts/BarChart.jsx` (×“×•×’××”)

```javascript
import { BarChart as RechartsBarChart, Bar, XAxis, YAxis, CartesianGrid, Tooltip, Legend } from 'recharts';

export const BarChart = ({ data, width = '100%', height = 400, colorScheme }) => {
  return (
    <RechartsBarChart width={width} height={height} data={data}>
      <CartesianGrid strokeDasharray="3 3" />
      <XAxis dataKey="name" />
      <YAxis />
      <Tooltip />
      <Legend />
      <Bar dataKey="value" fill={colorScheme?.primary || '#6366f1'} />
    </RechartsBarChart>
  );
};
```

---

## ğŸ¤– ×ª×”×œ×™×š Chart Transcription

### Flow ××œ×

```
User opens dashboard
    â†“
Charts render
    â†“
Startup transcription flow begins
    â†“
Fetch ALL charts (priority + BOX)
    â†“
Wait for charts to render (DOM)
    â†“
Build chart payloads (JSON)
    â†“
POST /api/v1/ai/chart-transcription/startup
    â†“
Backend processes charts sequentially
    â†“
For each chart:
    â†“
    Check if transcription exists in DB
    â†“
    If exists â†’ Skip (startup only creates if missing)
    â†“
    If not exists:
        â†“
        Call OpenAI API (JSON payload)
        â†“
        OpenAI returns transcription text
        â†“
        Save to database (UPSERT)
        â†“
        Wait 800ms (rate limiting)
    â†“
Next chart
    â†“
Return results to frontend
    â†“
Done!
```

### 1. Frontend - Startup Flow

**××™×§×•×:** `frontend/src/hooks/useDashboardData.js`

**×©×•×¨×” 229-360: Startup transcription**

```javascript
// Only run once per session
const isStartupDone = getStartupTranscriptionDone();
if (!isStartupDone) {
  // Mark as done immediately
  setStartupTranscriptionDone(true);
  
  // 1. Fetch ALL charts
  const allChartsResponse = await dashboardAPI.getAllCharts();
  const allChartsForTranscription = allChartsResponse.data?.charts || [];
  
  // 2. Wait for charts to render
  await waitForChartsStartup(20, 500); // Wait up to 10 seconds
  await new Promise(resolve => setTimeout(resolve, 2000)); // Additional 2 seconds
  
  // 3. Build payloads
  const chartsForStartup = [];
  for (const chart of allChartsForTranscription) {
    const chartPayload = buildChartTranscriptionPayload(chart);
    chartsForStartup.push({
      chartId: chart.id,
      context: chart.title,
      chartPayload
    });
  }
  
  // 4. Send to backend
  await chartTranscriptionAPI.startup(chartsForStartup);
}
```

**×©×•×¨×” 27-61: `buildChartTranscriptionPayload()`**

```javascript
const buildChartTranscriptionPayload = (chart) => {
  const dataArray = Array.isArray(chart.data) ? chart.data : [];
  const trimmedData = dataArray.slice(0, MAX_TRANSCRIPTION_POINTS).map(sanitizeDataPoint);
  
  return {
    chartId: chart.id || '',
    title: chart.title || '',
    type: chart.type || 'chart',
    axes: {
      x: metadata.xAxisLabel || null,
      y: metadata.yAxisLabel || null
    },
    seriesKeys: trimmedData.length > 0 
      ? Object.keys(trimmedData[0]).filter(key => key !== 'name' && typeof trimmedData[0][key] === 'number')
      : [],
    metadata: {
      services: metadata.services || [],
      colorScheme: metadata.colorScheme || null
    },
    data: trimmedData // Max 200 data points
  };
};
```

### 2. Backend - Startup Endpoint

**××™×§×•×:** `backend/src/presentation/routes/chartTranscription.js`

**×©×•×¨×” 104-503: `/chart-transcription/startup`**

```javascript
router.post('/chart-transcription/startup', async (req, res) => {
  const { charts } = req.body;
  
  // Validate
  if (!Array.isArray(charts) || charts.length === 0) {
    return res.status(400).json({ ok: false, error: 'charts[] required' });
  }
  
  const results = [];
  
  // Process charts sequentially (one at a time)
  for (let i = 0; i < charts.length; i++) {
    const c = charts[i];
    const { chartId, chartPayload, context } = c;
    
    try {
      // 1. Check if transcription already exists
      const existing = await findByChartId(chartId);
      
      if (existing && existing.transcription_text) {
        // Skip - already exists (startup only creates if missing)
        results.push({ chartId, status: 'skipped', reason: 'already_exists' });
        continue;
      }
      
      // 2. Add delay between charts (except first)
      if (i > 0) {
        await new Promise(resolve => setTimeout(resolve, 800));
      }
      
      // 3. Call OpenAI
      const text = await openaiQueue.enqueue(async () => {
        return await transcribeChartJson({ chartPayload, context });
      });
      
      // 4. Save to database
      const savedData = await upsertTranscriptionSimple({ chartId, text });
      
      results.push({ 
        chartId, 
        status: 'created',
        transcription_text: savedData.transcription_text
      });
    } catch (error) {
      results.push({ chartId, status: 'error', error: error.message });
    }
  }
  
  res.json({ ok: true, results });
});
```

### 3. OpenAI Service

**××™×§×•×:** `backend/src/application/services/transcribeChartService.js`

**×©×•×¨×” 127-199: `transcribeChartJson()`**

```javascript
export async function transcribeChartJson({ chartPayload, context }) {
  if (!openai) {
    // Mock fallback for development
    return `Chart Analysis\nâ€¢ This chart displays data trends...`;
  }
  
  const model = 'gpt-4o-mini'; // Lower cost, higher TPM limits
  
  // Serialize payload
  const serializedPayload = JSON.stringify(chartPayload, null, 2);
  
  // Call OpenAI
  const response = await withRetry(async () => {
    return await openai.chat.completions.create({
      model: model,
      messages: [
        {
          role: 'system',
          content: JSON_SYSTEM_PROMPT // Instructions for OpenAI
        },
        {
          role: 'user',
          content: `Context: ${context || 'Chart'}\n\nChart JSON:\n${serializedPayload}`
        }
      ],
      max_tokens: 400
    });
  }, 3); // Retry up to 3 times
  
  const text = response.choices[0]?.message?.content?.trim() || '';
  
  if (!text) {
    throw new Error('Empty transcription from OpenAI');
  }
  
  return text;
}
```

**System Prompt (×©×•×¨×” 1-33):**
```javascript
const JSON_SYSTEM_PROMPT = `You are an expert data analyst. Analyze the provided chart JSON data and provide a concise, insightful description.

Focus on:
- Key trends and patterns
- Notable data points
- Comparisons between series
- Business insights

Keep response under 400 tokens.`;
```

### 4. Save to Database

**××™×§×•×:** `backend/src/infrastructure/repositories/ChartTranscriptionsRepository.js`

**×©×•×¨×” 282-425: `upsertTranscriptionSimple()`**

```javascript
export async function upsertTranscriptionSimple({ chartId, text }) {
  const pool = getPool();
  const client = await pool.connect();
  
  try {
    // Compute signature (hash of chartId)
    const signature = computeChartSignature(chartId);
    
    // UPSERT (INSERT ... ON CONFLICT ... UPDATE)
    const result = await client.query(
      `INSERT INTO ai_chart_transcriptions 
       (chart_id, chart_signature, model, transcription_text, updated_at)
       VALUES ($1, $2, $3, $4, NOW())
       ON CONFLICT (chart_id) 
       DO UPDATE SET 
         transcription_text = EXCLUDED.transcription_text,
         updated_at = NOW()
       RETURNING *`,
      [chartId, signature, 'gpt-4o-mini', text]
    );
    
    return result.rows[0];
  } finally {
    client.release();
  }
}
```

### 5. Read Transcription

**××™×§×•×:** `backend/src/presentation/routes/chartTranscription.js`

**×©×•×¨×” 24-88: `GET /chart-transcription/:chartId`**

```javascript
router.get('/chart-transcription/:chartId', async (req, res) => {
  const chartId = req.params.chartId;
  
  try {
    // Query database
    const row = await findByChartId(chartId);
    
    // Always return 200 (never 404)
    if (!row) {
      return res.status(200).json({ 
        chartId,
        exists: false,
        transcription_text: null
      });
    }
    
    // Return transcription
    res.status(200).json({ 
      chartId: row.chart_id,
      exists: true,
      transcription_text: row.transcription_text
    });
  } catch (err) {
    // Error handling
    res.status(500).json({ 
      exists: false,
      transcription_text: null,
      error: err.message
    });
  }
});
```

---

## ğŸ”„ ×ª×”×œ×™×š Refresh Data

### Flow ××œ×

```
User clicks "Refresh Data" button
    â†“
POST /api/v1/dashboard/refresh
    â†“
DashboardController.refreshData()
    â†“
triggerManualCollection()
    â†“
CollectDataUseCase.execute()
    â†“
Fetch data from all microservices
    â†“
Save to database
    â†“
Return refresh status
    â†“
Frontend receives response
    â†“
Update charts with new data
    â†“
Wait for charts to render
    â†“
Refresh transcription flow begins
    â†“
POST /api/v1/ai/chart-transcription/refresh
    â†“
Backend processes charts sequentially
    â†“
For each chart:
    â†“
    Always call OpenAI (overwrite)
    â†“
    Save to database
    â†“
    Wait 800ms
    â†“
Next chart
    â†“
Done!
```

### 1. Frontend - Refresh

**××™×§×•×:** `frontend/src/hooks/useDashboardData.js`

**×©×•×¨×” 403-698: `refreshData()`**

```javascript
const refreshData = async (services) => {
  setRefreshing(true);
  
  // 1. Clear saved report from sessionStorage
  sessionStorage.removeItem('lastGeneratedReportData');
  
  // 2. Call backend
  const response = await dashboardAPI.refreshData(services);
  const dashboardData = response.data;
  
  // 3. Update state
  setData(dashboardData);
  setLastUpdated(dashboardData.lastUpdated);
  setRefreshStatus(dashboardData.refreshStatus || null);
  
  // 4. Wait for React to update DOM (3 seconds)
  await new Promise(resolve => setTimeout(resolve, 3000));
  
  // 5. Wait for charts to render
  await waitForCharts(20, 500);
  
  // 6. Additional wait to ensure charts show NEW data (3 seconds)
  await new Promise(resolve => setTimeout(resolve, 3000));
  
  // 7. Fetch ALL charts
  const allChartsResponse = await dashboardAPI.getAllCharts();
  const allChartsForTranscription = allChartsResponse.data?.charts || [];
  
  // 8. Build payloads and send to refresh endpoint
  const chartsForRefresh = allChartsForTranscription.map(chart => ({
    chartId: chart.id,
    context: chart.title,
    chartPayload: buildChartTranscriptionPayload(chart)
  }));
  
  // 9. Send to backend
  await chartTranscriptionAPI.refresh(chartsForRefresh);
};
```

### 2. Backend - Refresh Endpoint

**××™×§×•×:** `backend/src/presentation/routes/chartTranscription.js`

**×©×•×¨×” 311-503: `/chart-transcription/refresh`**

```javascript
router.post('/chart-transcription/refresh', async (req, res) => {
  const { charts } = req.body;
  
  const results = [];
  
  // Process charts sequentially
  for (let i = 0; i < charts.length; i++) {
    const c = charts[i];
    const { chartId, chartPayload, context } = c;
    
    try {
      // Add delay (except first)
      if (i > 0) {
        await new Promise(resolve => setTimeout(resolve, 800));
      }
      
      // ALWAYS call OpenAI (refresh always overwrites)
      const text = await openaiQueue.enqueue(async () => {
        return await transcribeChartJson({ chartPayload, context });
      });
      
      // Save to database (always overwrite)
      const savedData = await upsertTranscriptionSimple({ chartId, text });
      
      results.push({ 
        chartId, 
        status: 'updated',
        transcription_text: savedData.transcription_text
      });
    } catch (error) {
      results.push({ chartId, status: 'error', error: error.message });
    }
  }
  
  res.json({ ok: true, results });
});
```

**×”×‘×“×œ ×‘×™×Ÿ startup ×œ-refresh:**
- **Startup:** ×™×•×¦×¨ transcription ×¨×§ ×× ×œ× ×§×™×™×
- **Refresh:** ×ª××™×“ ×§×•×¨× ×œ-OpenAI ×•××¢×“×›×Ÿ (overwrite)

---

## ğŸ“„ ×ª×”×œ×™×š ×™×¦×™×¨×ª Reports

### Flow ××œ×

```
User navigates to /reports
    â†“
ReportsPage component mounts
    â†“
Fetch report types
    â†“
User selects report type
    â†“
Click "Generate Report"
    â†“
POST /api/v1/reports/generate
    â†“
GenerateReportUseCase.execute()
    â†“
Collect data from cache
    â†“
Generate charts for report
    â†“
Call OpenAI for report conclusions
    â†“
Generate PDF
    â†“
Return PDF blob
    â†“
Frontend displays PDF
    â†“
User can download
```

### 1. Frontend - Reports Page

**××™×§×•×:** `frontend/src/components/Reports/ReportsPage.jsx`

```javascript
const handleGenerateReport = async (reportType) => {
  setGenerating(true);
  
  try {
    // Call backend
    const response = await reportsAPI.generateReport(reportType, { format: 'pdf' });
    
    // Create blob URL
    const blob = new Blob([response.data], { type: 'application/pdf' });
    const url = window.URL.createObjectURL(blob);
    
    // Display in iframe
    setPdfUrl(url);
  } catch (error) {
    setError(error.message);
  } finally {
    setGenerating(false);
  }
};
```

### 2. Backend - Generate Report

**××™×§×•×:** `backend/src/presentation/routes/reports.js`

```javascript
router.post('/generate', async (req, res) => {
  const { reportType, format = 'pdf' } = req.body;
  
  // 1. Execute Use Case
  const reportData = await generateReportUseCase.execute(reportType);
  
  // 2. Generate PDF
  if (format === 'pdf') {
    const pdfBuffer = await pdfGenerator.generate(reportData);
    res.setHeader('Content-Type', 'application/pdf');
    res.setHeader('Content-Disposition', `attachment; filename="${reportType}.pdf"`);
    res.send(pdfBuffer);
  }
});
```

### 3. Generate Report Use Case

**××™×§×•×:** `backend/src/application/useCases/GenerateReportUseCase.js`

```javascript
async execute(reportType) {
  // 1. Get data from cache
  const latestEntries = await this.cacheRepository.getLatestEntries();
  
  // 2. Generate charts for report
  const charts = this.generateReportCharts(reportType, latestEntries);
  
  // 3. Call OpenAI for conclusions
  const conclusions = await this.reportConclusionsService.generate(
    reportType,
    charts
  );
  
  // 4. Return report data
  return {
    reportType,
    charts,
    conclusions,
    generatedAt: new Date().toISOString()
  };
}
```

---

## ğŸ” ×ª×”×œ×™×š AI Custom SQL

### Flow ××œ×

```
User navigates to /ai-custom
    â†“
User types natural language query
    â†“
Click "Generate graph"
    â†“
POST /api/ai-custom/query-data
    â†“
AICustomSqlService.generateSqlWithOpenAi()
    â†“
OpenAI generates SQL query
    â†“
Validate SQL safety (SELECT only, no dangerous keywords)
    â†“
Add LIMIT if missing
    â†“
Execute query against PostgreSQL
    â†“
Return results (columns, rows)
    â†“
Frontend transforms to chart data
    â†“
Display chart or table
```

### 1. Frontend - AI Custom Page

**××™×§×•×:** `frontend/src/components/AICustom/AICustomPage.jsx`

**×©×•×¨×” 284-342: `handleGenerate()`**

```javascript
const handleGenerate = async () => {
  const trimmedInput = userInput.trim();
  
  setLoading(true);
  
  try {
    // Call backend
    const response = await aiCustomAPI.queryData(trimmedInput);
    const data = response.data;
    
    // Handle different statuses
    if (data.status === 'no_match') {
      setError(data.message || 'No matching tables found');
      return;
    }
    
    if (data.status === 'ok') {
      // Transform to chart/table data
      const transformed = transformToChartData(data.columns, data.rows);
      
      setResult({
        sql: data.sql,
        reason: data.reason,
        rowCount: data.rowCount,
        columns: data.columns,
        rows: data.rows,
        transformed
      });
    }
  } catch (err) {
    setError(err.message);
  } finally {
    setLoading(false);
  }
};
```

**×©×•×¨×” 196-282: `transformToChartData()`**

```javascript
const transformToChartData = (columns, rows) => {
  // Empty result
  if (!rows || rows.length === 0) {
    return { kind: 'empty' };
  }
  
  // First column is X-axis (labels)
  const xCol = columns[0];
  const xKey = xCol.name;
  
  // Find all numeric columns beyond the first
  const numericColumns = [];
  for (let i = 1; i < columns.length; i++) {
    const col = columns[i];
    const sampleValues = rows.slice(0, 10).map(r => r[col.name]);
    const hasNumeric = sampleValues.some(v => isNumericValue(v));
    
    if (hasNumeric) {
      numericColumns.push(col);
    }
  }
  
  // No numeric columns -> table view
  if (numericColumns.length === 0) {
    return { kind: 'table', columns, rows };
  }
  
  // Extract labels and values
  const labels = rows.map(row => String(row[xKey] || ''));
  const series = numericColumns.map(col => ({
    key: col.name,
    label: col.name,
    values: rows.map(row => {
      const v = row[col.name];
      return typeof v === 'number' ? v : Number(v) || 0;
    })
  }));
  
  // Format chart data
  let chartData;
  if (series.length === 1) {
    // Single series
    chartData = labels.map((label, index) => ({
      name: label,
      value: series[0].values[index]
    }));
  } else {
    // Multiple series
    chartData = labels.map((label, index) => {
      const item = { name: label };
      series.forEach(s => {
        item[s.key] = s.values[index];
      });
      return item;
    });
  }
  
  return {
    kind: 'chart',
    xKey,
    labels,
    series,
    chartData,
    isMultiSeries: series.length > 1
  };
};
```

### 2. Backend - AI Custom Route

**××™×§×•×:** `backend/src/presentation/routes/aiCustom.js`

**×©×•×¨×” 226-355: `/query-data`**

```javascript
router.post('/query-data', async (req, res) => {
  const { queryText } = req.body;
  
  // 1. Validate input
  const validation = validateQueryText(queryText);
  if (!validation.valid) {
    return res.status(400).json({ status: 'error', message: validation.error });
  }
  
  // 2. Generate SQL using OpenAI
  const sqlResult = await aiCustomSqlService.generateSqlWithOpenAi(trimmedText);
  
  if (sqlResult.status === 'no_match') {
    return res.status(200).json({
      status: 'no_match',
      message: sqlResult.reason
    });
  }
  
  // 3. Validate SQL safety
  const safetyCheck = validateSqlSafety(sqlResult.sql);
  if (!safetyCheck.valid) {
    return res.status(400).json({
      status: 'error',
      message: 'SQL safety check failed'
    });
  }
  
  // 4. Add LIMIT if missing
  const safeSql = addLimitIfMissing(sqlResult.sql, 5000);
  
  // 5. Execute query
  try {
    const queryResult = await runAiCustomQuery(safeSql, 30000);
    
    return res.status(200).json({
      status: 'ok',
      sql: safeSql,
      reason: sqlResult.reason,
      rowCount: queryResult.rowCount,
      columns: queryResult.columns,
      rows: queryResult.rows
    });
  } catch (dbError) {
    return res.status(500).json({
      status: 'error',
      message: 'Query execution failed'
    });
  }
});
```

### 3. SQL Safety Validation

**××™×§×•×:** `backend/src/utils/sqlSafety.js`

**×©×•×¨×” 12-54: `validateSqlSafety()`**

```javascript
export function validateSqlSafety(sql) {
  // 1. Must be non-empty string
  if (!sql || typeof sql !== 'string' || sql.trim().length === 0) {
    return { valid: false, error: 'SQL query must be a non-empty string' };
  }
  
  const trimmed = sql.trim();
  
  // 2. Must start with SELECT
  const selectPattern = /^\s*select\b/i;
  if (!selectPattern.test(trimmed)) {
    return { valid: false, error: 'Only SELECT queries are allowed' };
  }
  
  // 3. Check for dangerous keywords
  const dangerousKeywords = [
    'insert', 'update', 'delete', 'drop', 'alter', 'truncate',
    'create', 'grant', 'revoke', 'execute', 'call', 'prepare'
  ];
  
  const upperSql = trimmed.toUpperCase();
  for (const keyword of dangerousKeywords) {
    const keywordPattern = new RegExp(`\\b${keyword}\\b`, 'i');
    if (keywordPattern.test(trimmed)) {
      return { valid: false, error: `Disallowed keyword found: ${keyword}` };
    }
  }
  
  // 4. Check for multiple statements
  const withoutTrailingSemicolon = trimmed.replace(/;\s*$/, '');
  if (withoutTrailingSemicolon.includes(';')) {
    return { valid: false, error: 'Multiple statements are not allowed' };
  }
  
  return { valid: true, error: null };
}
```

### 4. OpenAI SQL Generation

**××™×§×•×:** `backend/src/application/services/AICustomSqlService.js`

**×©×•×¨×” 31-131: `buildAiSqlPrompt()`**

```javascript
buildAiSqlPrompt(userText, migrationSql) {
  const systemMessage = {
    role: 'system',
    content: `You are an expert SQL generator for an analytics dashboard.

CRITICAL SAFETY RULES:
- You must only generate a **single PostgreSQL SELECT query**
- The query must be read-only: no INSERT, UPDATE, DELETE, DROP, ALTER
- Do not use temporary tables or stored procedures
- No multiple statements

MAPPING PHILOSOPHY:
- Always attempt to produce the most reasonable SQL query
- Prefer best-effort queries over "no_match"
- Make reasonable assumptions when information is missing

You receive:
- The full PostgreSQL schema from migration.sql
- A natural language request from the user

Generate the SQL query that best matches the user's request.`
  };
  
  const userMessage = {
    role: 'user',
    content: `User Request: ${userText}\n\nDatabase Schema:\n${migrationSql}`
  };
  
  return [systemMessage, userMessage];
}
```

---

## ğŸ“ ×¡×™×›×•× - × ×§×•×“×•×ª ××¤×ª×— ×œ×¨××™×•×Ÿ

### 1. ××¨×›×™×˜×§×˜×•×¨×”
- **Onion Architecture** - ×”×¤×¨×“×” ×‘×™×Ÿ ×©×›×‘×•×ª
- **Repository Pattern** - ×”×¤×¨×“×” ×‘×™×Ÿ business logic ×œ-data access
- **Use Cases** - ×›×œ business logic ×‘-use cases

### 2. ×ª×”×œ×™×›×™× ××¨×›×–×™×™×
- **Data Collection:** CRON jobs â†’ Microservices â†’ Database
- **Dashboard Loading:** Cache â†’ Use Case â†’ Format â†’ Frontend
- **Chart Transcription:** Frontend â†’ OpenAI â†’ Database
- **Reports:** Data â†’ Charts â†’ OpenAI â†’ PDF

### 3. ×˜×›× ×•×œ×•×’×™×•×ª
- **Frontend:** React + Vite + Recharts + TailwindCSS
- **Backend:** Node.js + Express + PostgreSQL
- **AI:** OpenAI GPT-4o-mini
- **Jobs:** node-cron

### 4. × ×§×•×“×•×ª ×—×©×•×‘×•×ª
- **Caching:** Browser cache + Database cache
- **Rate Limiting:** OpenAI queue + delays between requests
- **SQL Safety:** Validation ×œ×¤× ×™ execution
- **Error Handling:** Graceful degradation ×‘×›×œ ××§×•×

---

**××¡××š ×–×” ××›×™×œ ×”×¡×‘×¨ ××¤×•×¨×˜ ×¢×œ ×›×œ ×ª×”×œ×™×š ×‘××¢×¨×›×ª. ×›×œ ×§×˜×¢ ×§×•×“ ×›×•×œ×œ ××™×§×•× ××“×•×™×§ ×‘×§×•×“.**

**×‘×”×¦×œ×—×” ×‘×¨××™×•×Ÿ! ğŸš€**

